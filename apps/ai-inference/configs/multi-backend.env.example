# Multi-Backend Configuration (ai-base:latest)
# Supports auto-detection and selection of best GPU backend

# GPU Configuration (auto-detect by default)
GPU_BACKEND=auto
GPU_VARIANT=all

# CUDA Settings (for NVIDIA)
CUDA_VISIBLE_DEVICES=0
CUDA_HOME=/usr/local/cuda

# ROCm Settings (for AMD)
ROCM_PATH=/opt/rocm
HSA_OVERRIDE_GFX_VERSION=10.3.0
HIP_VISIBLE_DEVICES=0

# Vulkan Settings (universal fallback)
VK_LAYER_PATH=/opt/vulkan-sdk/etc/vulkan/explicit_layer.d
VULKAN_SDK=/opt/vulkan-sdk

# Model Paths
OLLAMA_MODELS=/models/ollama
LLAMA_MODELS=/models/llama
EXO_MODELS=/models/exo

# Logging
LOG_LEVEL=info

# Ollama Configuration
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_GPU_BACKEND=vulkan
OLLAMA_NUM_PARALLEL=1
OLLAMA_MAX_LOADED_MODELS=1

# EXO Configuration
EXO_MODE=standalone
EXO_DISCOVERY_PORT=5678

# Performance Tuning (universal)
OMP_NUM_THREADS=8
OPENBLAS_NUM_THREADS=8

# Backend Priority (when auto-detecting)
# Uncomment to override default priority (cuda > rocm > vulkan)
# BACKEND_PRIORITY=vulkan,cuda,rocm
